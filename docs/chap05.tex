\chapter{总结与展望}

\section{本研究工作总结}

在本文的研究中，我们探索了一种在最新的分布式计算框架和现代高性能硬件之间进行适配的性能优化。Plasma作为支撑分布式计算框架Ray的分布式内存管理组件，
在设计上并没有考虑过超算集群独特的硬件特性，特别是支持RDMA通信机制的高速网络，因而无法在超算集群上得到最佳的传输性能。我们首先通过跨节点的性能测试
验证了这一设想，特别是发现了其在大对象传输上存在明显的性能缺陷。

之后，面对Ray产生的大小不一的内存对象，我们利用RDMA设计了一种全面优于原实现的对象传输机制。针对小对象的网络传输，我们基于RDMA双边通信原语，设计了一种
低延迟的对象传输协议。该机制能够让接收方的本地访存操作和网络通信重叠，因此具有更优的传输延迟。此外，针对大对象的网络传输，我们使用RDMA单边通信原语设计了
一种兼具低延迟、高带宽的传输协议。该协议所需的通信次数较双边协议更少，并且实现了零拷贝特性，因此大大提升了Plasma在超算网络上的吞吐能力。我们的优化实现能根据
传输的数据大小从上述两种协议中选取合适的执行。

最后，我们对优化实现在各个大小的数据传输进行了性能测试。我们通过实验确定了混合传输机制的选择参数为32KB。双边协议在小于32KB大小的对象传输上实现了最低的
传输延迟。而对于大于32KB的数据对象，使用单边协议则提供了最优的传输带宽，能在4MB及以上大小的数据上提供接近一个数量级的性能提升。此时的Plasma相比Redis数据库，不仅
提供了分布式内存管理的支撑机制，在单节点上的传输性能依然具有明显优势。

\section{未来工作设想}

本文的工作在没有大幅度变动Plasma代码结构的前提下，实现了显著的性能优化——然而，针对分布式内存对象存储，仍然有很多可以探索的空间：

\begin{enumerate}
	\item 即便是采用了RDMA机制用作对象传输，Plasma管理者进程仍依赖Redis事件循环库\cite{ae}
	驱动函数调用。目前来看，这一部分仍未支持RDMA，从而限制了Plasma在多节点上的可扩展性。因此后续可以设计基于RDMA的RPC调用组件进一步重构Plasma，以提升其吞吐能力。
	\item 目前的Plasma实现和Ray的对象管理机制\cite{wang2021ownership}没有做到紧耦合，因而存在协同设计（co-design）的可能性。特别是引入支持RDMA特性的高性能网络后，我们是否能进一步设计：
	适应RDMA高速网络的内存对象管理机制，以及支撑该机制的高性能内存存储组件。
	\item 计算框架Ray面向的计算应用，例如强化学习等，大量地使用高性能计算卡（GPU）。然而，目前的Plasma只支持对主存中的数据对象进行存储和管理，而没有考虑GPU显存的存在。因此，在RDMA机制的帮助下，我们
	是否能将集群内GPU的内存空间纳入管理，并进一步针对性地优化内存管理机制，是非常值得研究的话题。
\end{enumerate}