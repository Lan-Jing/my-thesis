\chapter{Plasma分布式内存存储架构和性能分析}

\section{Plasma架构分析}

Plasma分布式存储架构由多个进程组成。通过将控制面、数据面上的任务解耦到不同的进程，我们可以较为方便地对其软件架构进行改进。
\autoref{fig:plasma_arch}展示了Plasma集群的组织结构。

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.75\textwidth]{image/chap02/plasma_arch.png}
    \caption{Plasma存储架构}
    \label{fig:plasma_arch}
\end{figure}

\subsection{Plasma对象存储架构}

对象存储（Object Store）是Plasma运行的最小单元，即Plasma在任意单节点上运行时，可以不需要连接到管理进程（Manager）和Redis数据库。
\autoref{fig:plasma_arch}中左下方部分展示了这一架构。对象存储的基本架构遵循用户端-服务端的特征。Store进程负责直接操作和管理机器内存。
当用户（Client）进程发起创建对象请求时，会将关键元数据——即对象编号（id）、元数据大小、数据大小等通过进程间通信（IPC）发送给Store进程。
值得注意的是，Plasma使用Unix域套接字（Unix Domain Socket）作为进程间通信方式，这一方式允许用户端以类似套接字的方式，通过操作系统内核与本机内的另一个进程交换数据。

Plasma利用MMAP映射机制对内存对象操作进行了性能优化。当前常见的内存数据库如Redis，用户端进程和服务端需要直接交换数据以完成创建、读取等操作。而Plasma利用了MMAP机制，
使得客户端与服务端能够共享内存空间：

\begin{enumerate}
    \item 创建对象时，服务端使用MMAP映射分配内存空间，将其与一临时文件映射。
    \item 服务端通过IPC，将上述临时文件的文件描述符（file descriptor，fd）传递给用户进程。
    \item 用户进程使用相同的文件描述符fd再次调用MMAP。
\end{enumerate}

如此，MMAP向Store进程和用户进程返回同一个内存地址，用户进程能够直接读取、操作数据库中的内存对象。这一架构能够将数据库读取操作的开销
优化到$O(1)$时间复杂度，使Plasma针对大型对象仍然具有优秀的吞吐能力。Plasma在对象存储架构中定义了如下操作原语：

\begin{table}[h]
    \centering
    \caption{Plasma客户端接口}
    \begin{tabular}{*{2}{c}}
        \toprule
        接口定义 & 接口描述      \\
        \midrule
        plasma\_contain(id)                                    & 查询对象是否存在   \\
        buf $\leftarrow$ plasma\_create(id, fields, values)    & 创建参数/数据为fields/values的对象   \\
        plasma\_seal(id)                                       & 封装对象使其对其他进程可见   \\
        plasma\_release(id)                                    & 释放对象，引用计数减一（为0则删除）   \\
        buf $\leftarrow$ plasma\_get(id, fields)               & 读取对象   \\
        plasma\_delete(id)                                     & 删除对象   \\
        \midrule
        plasma\_connect(store\_addr, manager\_addr)            & 连接到Store和Manager \\
        plasma\_subscribe(id)                                  & 等待一个对象在本机被创建 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:store_api}
\end{table}

\subsection{Plasma集群架构}

Plasma存储将其集群架构单独实现为Manager进程，多个进程以及一个Redis数据库共同构成其集群架构，\autoref{fig:plasma_arch}中上方
展示了将Plasma连接成集群的架构。Plasma在集群架构中定义了如下操作原语：

\begin{table}[h]
    \centering
    \caption{Plasma集群接口}
    \begin{tabular}{*{2}{c}}
        \toprule
        接口定义 & 接口描述      \\
        \midrule
        buflist $\leftarrow$ plasma\_fetch(ids)               & 将id列表中的对象拉取到本地存储   \\
        \bottomrule
    \end{tabular}
    \label{tab:manager_api}
\end{table}

管理者（Manager）进程仅处理用户进程发出的一种请求，即输入一个id列表，执行一次拉取（fetch）操作：

\begin{enumerate}
    \item Manager对列表中的每个id，从集群中查找对应数据对象的位置。 
    \item 本地Manager与存有这一对象的另一个Manager建立连接，拉取数据。
    \item Manager和Store交互将数据保存到本地——对Store进程来说，Manager只是普通的Client进程。
\end{enumerate}

在这一过程中，Plasma依赖于Redis提供的数据库服务。每当Client进程创建并封装数据对象时，它将借助Manager与Redis服务器建立通信，并更新该对象在集群中的分布。
因此Manager在拉取数据的过程中（上述步骤2），必须先从Redis得到数据分布，才能开始一次数据传输。

\section{基于套接字的Plasma通信机制}

Plasma实现了基于套接字（Socket）的数据传输机制。通过访问Redis服务器获得对象处在的目标节点后，Manager会逐一尝试建立Socket连接，并拉取数据。
其通信机制如\autoref{fig:sock_protocol}所示：

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.75\textwidth]{image/chap02/sock_protocol.png}
    \caption{基于套接字的Plasma通信机制}
    \label{fig:sock_protocol}
\end{figure}

发起者（Receiver）首先将一条类别为PLASMA\_TRANSFER的消息发送给存有数据对象的Manager进程。在得知需要发送的对象id后，发送方通过向Store调用查询操作获得了
数据的缓冲区地址。需要注意的是Plasma的读取操作具有常数的时间复杂度，因而会马上返回。发送方（Sender）会将查询到的元数据（主要是数据大小）通过PLASMA\_DATA消息返回给接收方，接收方便会
向Store创建一个同id的对象，获得分配的内存空间。最后双方分别进入发送/接收函数，分批将数据转移到本地缓冲区中。

在Plasma实现中，默认一次发送4kb大小的数据。

\section{Plasma存储和传输性能分析}

为了分析Plasma存储运行在超算上可能存在的性能瓶颈，我们在天河高性能集群上对Redis和Plasma进行了存储和传输上的性能测试。

\subsection{存储性能测试}

\subsection{数据传输测试}